# ğŸ›¡ï¸ TruthLens: Forensic Fake News Detection System

**TruthLens** is a multi-layered news verification platform that combines transformer-based deep learning (BERT), heuristic analysis (TF-IDF), topic modeling (BERTopic), and live cross-referencing (NLI-ready interface) to detect misinformation.

Unlike standard classifiers, TruthLens focuses on **forensic text analysis**â€”cleaning specific â€œdata artifactsâ€ (e.g., â€œ(Reuters)â€ headers or â€œClick hereâ€ buttons) so models learn **semantic untruths** rather than platform-specific metadata.

<p align="center">
  <img alt="TruthLens" src="https://img.shields.io/badge/python-3.10%2B-blue" />
  <img alt="License" src="https://img.shields.io/badge/license-MIT-green" />
  <img alt="Framework" src="https://img.shields.io/badge/ML-BERT%2C%20TF--IDF%2C%20BERTopic-orange" />
  <img alt="UI" src="https://img.shields.io/badge/UI-Streamlit-red" />
</p>

---

## âœ¨ Features

- **Forensic Cleaning**: Removes UI leaks, wire-service prefixes, metadata, and temporal markers to avoid shortcut learning.
- **Deep + Heuristic Stack**: BERT classifier + TF-IDF baseline + BERTopic clusters for interpretability.
- **Interactive Dashboard**: One-click Streamlit app with automatic model download on first run.
- **Reproducible Pipeline**: End-to-end scripts for dataset prep, training, evaluation, and topic visualization.
- **Portable Models**: Upload utility for pushing trained models to Hugging Face Hub.

---

## âš¡ Quick Start (Dashboard)

The easiest way to try TruthLens is via the interactive dashboard. On first run, it will auto-download any required models.

```bash
# 1) Create and activate a virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Launch the Command Center
streamlit run app.py
```

- The app opens at **http://localhost:8501**.
- Paste a news article and view:
  - Fake probability
  - Attention heatmaps
  - External fact-checking (NLI-style) cues

---

## ğŸ§­ Project Structure

```
TruthLens/
â”œâ”€â”€ data/                      # Dataset storage (e.g., balanced_dataset_200k.parquet)
â”œâ”€â”€ models/                    # Trained weights and checkpoints
â”œâ”€â”€ plots/                     # Generated evaluation figures
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Paths & hyperparameters
â”‚   â”œâ”€â”€ create_balanced_dataset.py  # Data balancing pipeline
â”‚   â”œâ”€â”€ train_bert_v2.py       # MAIN training (Forensic BERT, V12 Strict)
â”‚   â”œâ”€â”€ train_tfidf.py         # TF-IDF + Logistic Regression baseline
â”‚   â”œâ”€â”€ train_topics.py        # BERTopic training
â”‚   â”œâ”€â”€ evaluate_models.py     # Confusion matrices, ROC, calibration
â”‚   â”œâ”€â”€ visualize_topics.py    # Topic charts/exports
â”‚   â””â”€â”€ upload.py              # Hugging Face Hub uploader
â”œâ”€â”€ app.py                     # Streamlit dashboard
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Reproduction Pipeline

### 1) Data Preparation

A pre-processed dataset is included:

- `data/balanced_dataset_200k.parquet`

(Optional) Regenerate a balanced dataset from raw partitions:

```bash
python src/create_balanced_dataset.py
```

### 2) Model Training

**Forensic BERT (V2) â€” Primary Model**  
Trained with data without punctuation.

```bash
python src/train_bert_v1.py
# Output: models/bert_finetuned/
```

**Forensic BERT (V2) â€” Primary Model**  
Trained with **Strict** cleaning to remove artifacts.

```bash
python src/train_bert_v2.py
# Output: models/bert_final/
```

**Heuristic Baseline (TF-IDF)**  
Detects â€œlazyâ€ fakes via keyword patterns.

```bash
python src/train_tfidf.py
# Output: models/tfidf_logreg_robust.joblib
```

**Topic Model (BERTopic)**  
Clusters articles into semantic themes.

```bash
python src/train_topics.py
# Output: models/bertopic_model/
```

### 3) Evaluation & Metrics

Generate confusion matrices, ROC curves, calibration plots, and summaries:

```bash
python src/evaluate_models.py
# Results in: plots/ (e.g., cm_new_bert.png, roc_*.png)
```

### 4) Topic Visualization

Export static visualizations for topic clusters:

```bash
python src/visualize_topics.py
# Results in: plots/topics_*.png
```

---

## ğŸ§ª Forensic Cleaning (V12 Strict)

To prevent metadata leakage and date-based shortcuts, we remove:

- **UI Leaks**: â€œClick hereâ€, â€œJoin our newsletterâ€, â€œView Galleryâ€
- **Agency Headers**: Reuters, AP, AFP, CNN, etc.
- **Metadata**: â€œPhoto creditâ€, â€œEditorâ€, â€œReporting byâ€
- **Temporal Markers**: Month names (January, February, â€¦)

Implementation details live in `train_bert_v2.py` and shared utilities referenced by `config.py`.

---

## â˜ï¸ Model Deployment & Portability

Use `src/upload.py` to push trained models to the Hugging Face Hub. This lets the Streamlit app fetch them automatically on any machine.

```bash
# Prerequisite: export your HF write token securely
export HUGGINGFACE_HUB_TOKEN=hf_xxx

# Upload your final model(s)
python src/upload.py
```

**What it does:**

- Pushes `models/bert_final/` (and any other configured paths) to your specified repositories.

> ğŸ”’ **Security Tip:** Do **not** hard-code tokens in `config.py`. Use environment variables or your CI secret store.  
> âœ… Pin the owner in `repo_id`, e.g. `"YourUser/fake-news-bert-v2"`, so uploads never go to the wrong account.

---

## ğŸš€ CLI Recipes

**Train + Evaluate end-to-end**

```bash
python src/train_bert_v2.py && python src/train_bert_v2.py  && python src/train_tfidf.py  && python src/train_topics.py  && python src/evaluate_models.py
```

**Run Dashboard after training**

```bash
streamlit run app.py
```

**Upload trained models**

```bash
export HUGGINGFACE_HUB_TOKEN=hf_xxx
python src/upload.py
```

---

## ğŸ§© Dependencies

Install from the pinned requirements:

```bash
pip install -r requirements.txt
```

Typical stack (subset): `transformers`, `torch`, `huggingface_hub`, `scikit-learn`, `pandas`, `numpy`, `bertopic`, `umap-learn`, `matplotlib`, `streamlit`.

---

## ğŸ§° Troubleshooting

- **403/401 during upload**  
  Ensure `HUGGINGFACE_HUB_TOKEN` has write scope and that `repo_id` is in your namespace (e.g., `YourUser/...`) or youâ€™re a collaborator.

- **Models not found in the app (first run)**  
  Check internet connectivity and that model repo names match `config.py`. The app downloads on demand.

- **CUDA Out of Memory**  
  Lower `BATCH_SIZE`, reduce `MAX_LEN`, or run on CPU by setting `CUDA_VISIBLE_DEVICES=""`.

- **Topic modeling errors**  
  Ensure `umap-learn` and `hdbscan` are installed (if you use HDBSCAN-based clustering).

---

## ğŸ“œ License

This project is released under the **MIT License**. See `LICENSE` for details.

---

## ğŸ™Œ Acknowledgments

- Hugging Face Transformers & Hub
- Streamlit
- BERTopic

---

## ğŸ§‘ğŸ»â€ğŸ’» Author

Simone De Giorgi - [GitHub](https://github.com/simo-dg) | [LinkedIn](https://www.linkedin.com/in/simone-de-giorgi/)
